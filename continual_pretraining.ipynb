{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SLrA4ubzgqOG",
        "outputId": "0089fc21-649f-48c7-efcd-945eb98c4676"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate\n",
            "  Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate\n",
            "  Downloading accelerate-0.32.1-py3-none-any.whl (314 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Collecting requests (from transformers)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.1+cu121)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: xxhash, requests, pyarrow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets, evaluate, accelerate\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.32.1 datasets-2.20.0 dill-0.3.8 evaluate-0.4.2 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 pyarrow-17.0.0 requests-2.32.3 xxhash-3.4.1\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "3b69468e51b040b4bc3200da624387da",
              "pip_warning": {
                "packages": [
                  "pyarrow",
                  "requests"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install transformers datasets evaluate accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kycyEN6CY6Fm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import Trainer, TrainingArguments, BertTokenizer, BertForMaskedLM, AutoTokenizer\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import load_dataset, DatasetDict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Continual pretraining**"
      ],
      "metadata": {
        "id": "61NCfjRvo2ip"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uHlUnVykDJ5"
      },
      "outputs": [],
      "source": [
        "# try domain related data with different sizes\n",
        "with open('/content/drive/MyDrive/dsir/out_dir_300m 100ksample/german_drama_100k_letters.txt', 'r', encoding='utf-8') as file:\n",
        "    raw_text = file.read()\n",
        "texts = raw_text.split('\\\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ug_oz5Ka3hs",
        "outputId": "f8a3f68d-3c53-4ab1-8ba4-02569e79d6ea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['\"Ferah Ulucay, die 23-jährige Generalsekretärin des Zentralrats, begrüsste mich freundlich und duzte mich. Ich fragte sie, wie sie eigentlich zum Zentralrat gekommen sei. Ich stamme aus einer kurdischen Familie, sagte sie. Ich bin in Bern aufgewachsen, völlig frei. Aber etwas stimmte für mich nicht. Alkohol trinken, Partys, Männer, das alles erschien mir als falsch. So fand ich zur Religion. Der Zentralrat war die einzige Anlaufstelle, als ich Hilfe suchte, nachdem es wegen der Konversion zum Islam zum kurzzeitigen Konflikt mit meinen Eltern kam.',\n",
              " '\"\"Was ist das?, will er wissen. Insgeheim hofft er wohl auf ein Ründchen Poker. Die legt Karten., sage ich. Versteht er nicht. Also erkläre ich: Sie kann dir die Zukunft voraussagen. Wann du den ersten Kuss bekommst, wann du deine große Liebe triffst Das ist dem jungen Herrn Peppinello peinlich. Außerdem unterbricht die Kartenlegerin mich ziemlich ungehalten. Nein. Also solchen Quatsch machen wir hier nicht. Sie ist böse. Es ist also Ernst. Ach so, entgegne ich, was denn dann sonst? (Ich bin wirklich versucht sie zu fragen ob sie Frauke ist. Aber.',\n",
              " '\"\"Ich will Ihnen die Geschichte ein andermal erzählen, Mayordomo, sagte er finster, denn sie ist wohl werth, daß sie von Allen gehört wird, die jetzt im Begriff stehen, die Apachen zu bekämpfen. Sie wird Sie lehren, die tausend Listen dieser schwarzen Teufel nicht zu gering anzuschlagen, und sich vor ihnen zu hüten, wenn Sie Ihren Scalp auf dem Kopfe behalten wollen. Heute, Monsieur, erlassen Sie es mir, denn ich bin, durch Ihre Erzählung an mein Unglück erinnert, nicht in der Stimmung, davon zu reden.',\n",
              " '\"\"Bueno! Sie müssen wissen, daß ich die Zeit, die ich nicht auf der Insel im Dienst der Plazer Compaa von Guyamas zubrachte, deren Oberaufseher mein Vater war, mich in den Küstenstädten des Festlandes umhertrieb, um das Geld, das ich gewonnen, wieder los zu werden. Ich tanzte mit den China\\'s, brachte den vornehmeren Damen Serenaden, war ein Spieler und lustiger Gesell und mit aller Welt in Freundschaft, so lange ich keine Ursache hatte, mein Messer zu ziehen, was allerdings ziemlich oft vorkam, da ich die Schwäche habe, eine Beleidigung nicht eher vergessen zu können, als bis ich mich dafür revangirt habe.',\n",
              " '\"\"Ta ta! schrie der alte Unhold, ihr Vater, der wahrhaftig kein Herz und keine Seele hatte, dazwischen - es ist schade, daß Don Riccardo todt ist; denn er war ein sehr freigebiger Mann und hatte eine glückliche Hand im Perlenkaufen. Ich hätte Nichts dawider gehabt, wenn Du ihn geheirathet hättest; aber da er todt ist, haben wir Nichts mehr mit ihm zu schaffen. Vamos! pack geschwind Deine besten Sachen zusammen, damit die Spitzbuben, wenn sie kommen, Nichts zu plündern finden, und dann wollen wir berathen, was zu thun ist!']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "texts[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYbr88RoSQQd",
        "outputId": "24da911b-a7d0-487d-d416-f3170e5392fb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# use gbert-large for german literature\n",
        "tokenizer = BertTokenizer.from_pretrained('deepset/gbert-large')\n",
        "encodings = tokenizer(texts, truncation=True, padding=True, max_length=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2OSp0eElLGq"
      },
      "outputs": [],
      "source": [
        "def mask_tokens(inputs, tokenizer, mlm_probability=0.15):\n",
        "    labels = inputs.clone()\n",
        "    probability_matrix = torch.full(labels.shape, mlm_probability)\n",
        "    special_tokens_mask = [\n",
        "        tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
        "    ]\n",
        "    probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
        "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
        "    labels[~masked_indices] = -100\n",
        "    inputs[masked_indices] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
        "\n",
        "    return inputs, labels\n",
        "\n",
        "input_ids = torch.tensor(encodings['input_ids'])\n",
        "inputs, labels = mask_tokens(input_ids, tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gM8Ay_8Spk4"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=32,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        "    prediction_loss_only=True,\n",
        "    learning_rate=4e-5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbrOie38SsWZ"
      },
      "outputs": [],
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "dataset = TextDataset(encodings, labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49M_cGi2mGYy",
        "outputId": "29cf2f18-ce25-4136-9ecc-d953d28b250e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at deepset/gbert-large were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "model = BertForMaskedLM.from_pretrained('deepset/gbert-large')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NthO3jt8UE2w"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 942
        },
        "id": "o64lRbAMUHXL",
        "outputId": "7ce872c6-df64-47d1-8735-3123b71f91c8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-b880be01e38c>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item['labels'] = torch.tensor(self.labels[idx])\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6210' max='15630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 6210/15630 2:47:08 < 4:13:37, 0.62 it/s, Epoch 1.99/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.082400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.006600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.046700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.028600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.102900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.007700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.002700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.002200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.001200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.002600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.001300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.001400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZBTmVIEUJMg"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained('/content/drive/MyDrive/dsir/gbert-large-continued-100k')\n",
        "tokenizer.save_pretrained('/content/drive/MyDrive/dsir/gbert-large-continued-100k')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Downstrem tasks**"
      ],
      "metadata": {
        "id": "_HbGQbZuouIr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XjfnZEMGd05"
      },
      "outputs": [],
      "source": [
        "dataset1 = load_dataset('csv', data_files={'train': '/content/drive/MyDrive/dsir/continuous training/drama_polarity_filtered.csv'})\n",
        "dataset2 = load_dataset('csv', data_files={'train': '/content/drive/MyDrive/dsir/continuous training/drama_main_class_filtered.csv'})\n",
        "dataset3 = load_dataset('csv', data_files={'train': '/content/drive/MyDrive/dsir/continuous training/drama_sub_emo_filtered.csv'})\n",
        "\n",
        "def data_split(dataset):\n",
        "    train_test_split = dataset['train'].train_test_split(test_size=0.2)\n",
        "    return DatasetDict({\n",
        "        'train': train_test_split['train'],\n",
        "        'test': train_test_split['test']\n",
        "    })\n",
        "\n",
        "dataset1_splited = data_split(dataset1)\n",
        "dataset2_splited = data_split(dataset2)\n",
        "dataset3_splited = data_split(dataset3)\n",
        "\n",
        "print(dataset1_splited)\n",
        "print(dataset2_splited)\n",
        "print(dataset3_splited)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NkRxvahPqI8"
      },
      "outputs": [],
      "source": [
        "# check every type of annotation for each dataset\n",
        "\n",
        "def get_unique_values(dataset, column_name):\n",
        "    column_values = dataset['train'][column_name]\n",
        "    unique_values = set(column_values)\n",
        "\n",
        "    return unique_values\n",
        "\n",
        "column1 = get_unique_values(dataset1, 'polarity')\n",
        "column2 = get_unique_values(dataset2, 'main_emotion_class')\n",
        "column3 = get_unique_values(dataset3, 'tag_type')\n",
        "\n",
        "print(column1)\n",
        "print(column2)\n",
        "print(column3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sN6Wq1TmSPzG"
      },
      "outputs": [],
      "source": [
        "# mapping method for dataset 1\n",
        "def label_mapping_1(data):\n",
        "    label_map = {\n",
        "        'positiv':0,\n",
        "        'negativ':1,\n",
        "        'gemischt':2,\n",
        "        'Emotionale Bewegtheit':3\n",
        "    }\n",
        "    data['label'] = label_map[data['polarity']]\n",
        "    del data['polarity']\n",
        "    return data\n",
        "\n",
        "dataset1_splited = dataset1_splited.map(label_mapping_1)\n",
        "\n",
        "# mapping method for dataset 2\n",
        "def label_mapping_2(data):\n",
        "    label_map = {\n",
        "        'Emotionen der Freude':0,\n",
        "        'Emotionen der Angst':1,\n",
        "        'Emotionen der Ablehnung':2,\n",
        "        'Emotionale Bewegtheit':3,\n",
        "        'Emotionen des Leids':4,\n",
        "        'Emotionen der Zuneigung':5\n",
        "    }\n",
        "    data['label'] = label_map[data['main_emotion_class']]\n",
        "    del data['main_emotion_class']\n",
        "    return data\n",
        "dataset2_splited = dataset2_splited.map(label_mapping_2)\n",
        "\n",
        "# mapping method for dataset 3\n",
        "def label_mapping_3(data):\n",
        "    label_map = {\n",
        "        'Verehrung':0,\n",
        "        'Angst':1,\n",
        "        'Leid':2,\n",
        "        'Liebe':3,\n",
        "        'Freude':4,\n",
        "        'Emotionale Bewegtheit':5,\n",
        "        'Mitleid':6,\n",
        "        'Abscheu':7,\n",
        "        'Ärger':8,\n",
        "        'Freundschaft':9,\n",
        "        'Lust':10,\n",
        "        'Verzweiflung':11,\n",
        "        'Schadenfreude':12\n",
        "    }\n",
        "    data['label'] = label_map[data['tag_type']]\n",
        "    del data['tag_type']  # Remove the old label column if not needed\n",
        "    return data\n",
        "\n",
        "dataset3_splited = dataset3_splited.map(label_mapping_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTd_tWwrGeAd"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"deepset/gbert-large\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyT96DPPGeEh"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True)\n",
        "\n",
        "tokenized_dataset1 = dataset1_splited.map(preprocess_function, batched=True)\n",
        "tokenized_dataset2 = dataset2_splited.map(preprocess_function, batched=True)\n",
        "tokenized_dataset3 = dataset3_splited.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdMJVvkW9Wg4"
      },
      "outputs": [],
      "source": [
        "sample_text = tokenized_dataset1[\"train\"][5][\"text\"]\n",
        "tokenized_sample = tokenizer(sample_text)\n",
        "\n",
        "print(\"Original Text:\", sample_text)\n",
        "print(\"Tokenized Text:\", tokenized_sample)\n",
        "print(\"Decoded Tokens:\", tokenizer.decode(tokenized_sample['input_ids']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gT2ZeFuTKKH"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3duPOpFiTKV3"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "accuracy = evaluate.load(\"accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_6XqqqrTKcz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return accuracy.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypcXxvl5TRXX"
      },
      "outputs": [],
      "source": [
        "id2label = { 0:'positiv', 1:'negativ', 2:'gemischt', 3:'Emotionale Bewegtheit'}\n",
        "label2id = {'positiv':0,'negativ':1,'gemischt':2,'Emotionale Bewegtheit':3}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRZdKUkeZPNf"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"/content/drive/MyDrive/dsir/gbert-large-continued-100k\", num_labels=4, id2label=id2label, label2id=label2id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLBXjk27ad6x"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['HF_API_TOKEN'] = \"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\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8hjgdjOar0O"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(token=\"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\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcuYhxP0eHl3"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7MqTte8-TRcj",
        "outputId": "8643d963-1fab-4039-ca8d-be3adc76de23"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1272' max='1272' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1272/1272 12:22, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.634473</td>\n",
              "      <td>0.752853</td>\n",
              "      <td>0.745654</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.711700</td>\n",
              "      <td>0.543910</td>\n",
              "      <td>0.798898</td>\n",
              "      <td>0.799321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.711700</td>\n",
              "      <td>0.640683</td>\n",
              "      <td>0.787092</td>\n",
              "      <td>0.787261</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.345400</td>\n",
              "      <td>0.710637</td>\n",
              "      <td>0.811492</td>\n",
              "      <td>0.809504</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1272, training_loss=0.45350167136522207, metrics={'train_runtime': 742.5429, 'train_samples_per_second': 54.731, 'train_steps_per_second': 1.713, 'total_flos': 9259353999305856.0, 'train_loss': 0.45350167136522207, 'epoch': 4.0})"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"model_polarity_100k\",\n",
        "    learning_rate=4e-5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=4,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset1[\"train\"],\n",
        "    eval_dataset=tokenized_dataset1[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsQ8aZVhTRfn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Q4y4EQ-JAYgh"
      },
      "outputs": [],
      "source": [
        "id2label = { 0:'Emotionen der Freude',\n",
        "            1:'Emotionen der Angst',\n",
        "             2:'Emotionen der Ablehnung',\n",
        "             3:'Emotionale Bewegtheit',\n",
        "             4:'Emotionen des Leids',\n",
        "             5:'Emotionen der Zuneigung'}\n",
        "label2id = {'Emotionen der Freude':0,\n",
        "        'Emotionen der Angst':1,\n",
        "        'Emotionen der Ablehnung':2,\n",
        "        'Emotionale Bewegtheit':3,\n",
        "        'Emotionen des Leids':4,\n",
        "        'Emotionen der Zuneigung':5}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "70LguB4NAYgi",
        "outputId": "aa4423f3-a209-492c-9392-0156ebdb7323"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/dsir/gbert-large-continued-100k and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"/content/drive/MyDrive/dsir/gbert-large-continued-100k\", num_labels=6, id2label=id2label, label2id=label2id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FoCR8Y8PAYgi"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpjWkewVAYgi"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"model_main_class_100k\",\n",
        "    learning_rate=4e-5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=4,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset2[\"train\"],\n",
        "    eval_dataset=tokenized_dataset2[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81j7jIwuTRh8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pdO_wptnR9Cv"
      },
      "outputs": [],
      "source": [
        "id2label = { 0:'Verehrung',\n",
        "            1:'Angst',\n",
        "             2:'Leid',\n",
        "             3:'Liebe',\n",
        "             4:'Freude',\n",
        "             5:'Emotionale Bewegtheit',\n",
        "             6:'Mitleid',\n",
        "             7:'Abscheu',\n",
        "             8:'Ärger',\n",
        "             9:'Freundschaft',\n",
        "             10:'Lust',\n",
        "             11:'Verzweiflung',\n",
        "             12:'Schadenfreude'}\n",
        "\n",
        "label2id = {'Verehrung':0,\n",
        "        'Angst':1,\n",
        "        'Leid':2,\n",
        "        'Liebe':3,\n",
        "        'Freude':4,\n",
        "        'Emotionale Bewegtheit':5,\n",
        "        'Mitleid':6,\n",
        "        'Abscheu':7,\n",
        "        'Ärger':8,\n",
        "        'Freundschaft':9,\n",
        "        'Lust':10,\n",
        "        'Verzweiflung':11,\n",
        "        'Schadenfreude':12}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "04cgmj76R5E7"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"/content/drive/MyDrive/dsir/gbert-large-continued-100k\", num_labels=13, id2label=id2label, label2id=label2id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pWYuUf7kR5E8"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JaDvP5ziR5E8"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"model_sub_emo_100k\",\n",
        "    learning_rate=4e-5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=4,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset3[\"train\"],\n",
        "    eval_dataset=tokenized_dataset3[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIr5yXjjSlCd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KEho7MhWDg4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2Kqrva4WzH3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}